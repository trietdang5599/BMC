tokenizer: Qwen/Qwen2.5-0.5B-Instruct
plm: Qwen/Qwen2.5-0.5B-Instruct
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
max_sequence_length: 1024
cached_dir: "cache/PRO_LLM/"
weight_decay: 0.005
learning_rate: 0.0005 
num_warmup_steps: 3000
gradient_accumulation_steps: 1
num_train_epochs: 5
dropout: 0.1
max_grad_norm: 5
run_sft: true
run_rlt: true
run_online_eval: true
run_offline_eval: true
run_iterative_finetuning: false
num_train_rl_epochs: 10
sampled_times: 100
rl_learning_rate: 0.00001