max_gen_length: 20
temperature: 0.0
tokenizer: Qwen/Qwen3-8B
plm: Qwen/Qwen3-8B
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
max_sequence_length: 1024
cached_dir: "cache/Finetuned_LLM/"
weight_decay: 0.005
learning_rate: 0.0005
num_warmup_steps: 3000
gradient_accumulation_steps: 1
num_train_epochs: 1
dropout: 0.1
max_grad_norm: 5
run_sft: false
run_offline_eval: false
